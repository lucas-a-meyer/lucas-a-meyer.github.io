[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts-sample/welcome/index.html",
    "href": "posts-sample/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts-sample/post-with-code/index.html",
    "href": "posts-sample/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lucas A. Meyer’s Blog",
    "section": "",
    "text": "Jun 10, 2022\n\n\nLucas A. Meyer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2022\n\n\nLucas A. Meyer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2019\n\n\nLucas A. Meyer\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2019\n\n\nLucas A. Meyer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2019\n\n\nLucas A. Meyer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2018\n\n\nLucas A. Meyer\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2018\n\n\nLucas A. Meyer\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2018\n\n\nLucas A. Meyer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2017\n\n\nLucas A. Meyer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2017\n\n\nLucas A. Meyer\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2018-05-28-a-reflection-on-fy18-and-fy19-plans.html",
    "href": "posts/2018-05-28-a-reflection-on-fy18-and-fy19-plans.html",
    "title": "A reflection on FY18 and FY19 plans",
    "section": "",
    "text": "I have worked at Microsoft for long enough that it’s easier for me to make plans in tandem with the Microsoft fiscal calendar. It runs from July to June, so we’re very close to the end of FY18 and starting FY19.\n\n\nLooking back, FY18 was a year of recovery, as was FY17. We’ve had our share of health-related scares at my home, and we’re still working through some of those. However, on the positive side, it finally feels like I have some time to think (and have hope) about the future.\nI also got a promotion right at the start of FY18, mostly because of my chatbot.\n\n\nIn July 2018, the very beginning of FY18, a chatbot I wrote in FY17 won the Adam Smith Judge’s Choice Award. That’s not me in the picture, by the way If I were to advertise this chatbot, I’d say its main feature is how fast it was created. It was written in about a week. Its main business feature is that it saves time by putting together information that took a few queries (and more than a few minutes) to combine. Although the chatbot could be replaced by a well-done report, it’s easier to train user to use a chatbot (the chatbot itself helps) than to use a report. This chatbot later became a platform for additional use cases.\nThis chatbot was the inspiration, and in some cases, the initial source code, for several other chatbots, including another one that won the Adam Smith award in 2018. One of the coolest features of this new chatbot is that it will ask the old chatbot when it doesn’t know the answer to a question.\n\n\n\nIn another short project, I’ve created a time series forecasting platform that is used by several groups. Although it does not use super-recent time series forecasting algorithms and has quite a few restrictions on the data it can handle, it is general enough to solve a wide range of problems that we commonly face, so it was adopted by several groups.\n\n\n\nIn large companies, data can always be combined in smarter and more useful ways. In FY18 we combined such data for two large opportunities: one to help streamline sales, and another one for data mastering. Both were widely adopted.\n\n\n\nIn April of 2018, one of my project proposals was accepted to the AI-611 Advanced Projects course, an internal course that selects top engineers and teaches them advanced concepts in machine learning, mostly related to deep learning. The project was completed in less than two months and was recently put in production.\n\n\n\nWith all the healthcare uncertainty, I had been out of conferences for a while. The last one I had presented was an internal conference in FY16. It’s time for a change, so I’m presenting in Baden-Baden in June 2018.\nOn the positive side, I’ve attended the Data Science Salon in Miami, and it was way better than I expected it to be.\n\n\n\n\n\n\nFor many years, I’ve been (perhaos incorrectly) proud of not being in the Data Science track at Microsoft. I was able to keep my career as “Finance”. Apparently this will change in FY19, and perhaps it’s for the better. One consequence is that I’ll be expected to publish some articles and present more in conferences.\n\n\n\nThere are advantages and disadvantages of being in a place that is a work in progress for Data Science, like Florida currently is. One of the advantages is that I can help shape how Data Science in Florida will be, and I can know many of the people in Florida that do Data Science. One disadvantage is that there’s not that many people to talk to. Florida is also very spread out, so it’s not uncommon that when something happens, it will be 90 minutes away and of uncertain quality.\nAnother disadvantage of Florida is that where we live (Broward County) doesn’t seem to be an ideal place for K-12, and we are worried about the effect on the kids.\nPerhaps there’s a place with better K-12 and more Data Science in our future. Seattle would make sense, given it’s where Microsoft is, but it’s expensive enough to give us pause.\n\n\n\nAs part of my likely move to a Data Science career profile, I’ll need to write papers and submit talks to conferences. Today I’ve created my EasyChair account, but I was not fast enough to realize that most of 2018 is already gone. I’m likely going to need to create several proposals so I can have some acceptances in early 2019.\nSo far the ideas are: * The (super-cheap) Twitter Art Bot * Chatbots that talk to each other * Something about our deep learning project fro AI-611 * Something about time-series forecasting\n\n\n\nCreativity is like breathing. You got to breath out, and you got to breath in. It may be time to breath out again, This coming fiscal year, I hope to have:\n\n24 blog posts, that is, at least two per month\n4 presentations in local meetups\n2 external conference presentations"
  },
  {
    "objectID": "posts/moved-linkedin-posts.html",
    "href": "posts/moved-linkedin-posts.html",
    "title": "Moved all my LinkedIn posts",
    "section": "",
    "text": "I moved all my LinkedIn posts that were on the blog to a new section."
  },
  {
    "objectID": "posts/moved-linkedin-posts.html#the-discipline-of-writing-about-what-youre-doing",
    "href": "posts/moved-linkedin-posts.html#the-discipline-of-writing-about-what-youre-doing",
    "title": "Why you should blog if you are a data scientist",
    "section": "The discipline of writing about what you’re doing",
    "text": "The discipline of writing about what you’re doing\nWhen I was a wee little kid and had just entered college, one of my first classes was “Physics Lab”, and the first class of that was to measure “gravity”, more specifically, the gravitational acceleration . Of course, mathy Computer Science studies that we were, we all knew that the gravitational acceleration would be $ g m/s^2 $.\nMeasuring it, however, it’s not very easy. First, remember that this is in the early 90s in Brazil, so digital cameras were very rare. Part of the problem is that 9.8 meters (approximately 30 feet) is quite high, so if we wanted our experiment to take around 1s, we would need a big ladder. Or, as it happened, we’d need to run an experiment that took less than a second for each run. In a class with 25 students, that was the preferred route.\nWe set up a vertical track attached to a device that would spark every 1/60 of a second, and we attached grid paper to the track. This is called a Behr free fall apparatus.\n\n\n\nBehr Free Fall Apparatus\n\n\nWe would release the device from the top of the track, and the sparks would mark the grid paper. We would then manually measure the distances between the sparks and the differences between the distances would tell us the acceleration. In theory. Again, remember this is the early 90s, so there’s no Windows 95 or Excel easily available, there were several steps that were prone to error.\nThe desired outcome was not only to calculate the acceleration due to gravity, but also to generate a lab report."
  },
  {
    "objectID": "posts/moved-linkedin-posts.html#writing-the-lab-report",
    "href": "posts/moved-linkedin-posts.html#writing-the-lab-report",
    "title": "Why you should blog if you are a data scientist",
    "section": "Writing the lab report",
    "text": "Writing the lab report\nIt was a simple experiment, but there was a twist. Another class of 25 students would have to replicate the experiment following the lab report from the first 25. Oh boy. We quickly found out that the best lab reports were the ones in which the experimenter would document the experiment as they were executing the experiment. Another thing that worked was going through the experiment more than once. What did not work was to perform the experiment and then go to where the computers were to type up a report from memory.\nI thought that was very insightful, shortly afterwards, we would learn about Donald Knuth’s proposed paradigm of literate programming, which lives on in Jupyter Notebooks and R Markdown and that is very popular in data science today. Behind it is the same concept of explanations interspersed with technical work to produce something that is reproducible and easy to understand."
  },
  {
    "objectID": "posts/moved-linkedin-posts.html#blogging-helps-you-train-to-write-about-your-work",
    "href": "posts/moved-linkedin-posts.html#blogging-helps-you-train-to-write-about-your-work",
    "title": "Why you should blog if you are a data scientist",
    "section": "Blogging helps you train to write about your work",
    "text": "Blogging helps you train to write about your work\nBesides the excellent reasons offered by David Robinson for aspiring data scientists to blog, I think experienced data scientists can also benefit from blogging. David argues that aspiring data scientists should blog for technical practice, to build a portfolio, and to get feedback. Experienced data scientists might benefit for similar reasons, but not exactly the same.\nSimilar to “practicing”, blogging about technical topics helps you learn how to communicate better about them, and also helps you learn how to write better research reports. Similar to “portfolio”, it signals a data scientist’s body of work. For an early-in-career data scientist, it may be more showing that you can do analysis, but for an experienced data scientist is more about what type of analysis you like to do. Finally, I think that “feedback” also works differently for early-in-career and experienced data scientists. I’m not sure random strangers in the internet will sweep in and correct your posts, but you can find out what people are reading and sharing. You may be surprised when you learn what parts of your work people find interesting."
  },
  {
    "objectID": "posts/2019-02-04-a-story-about-my-father.html",
    "href": "posts/2019-02-04-a-story-about-my-father.html",
    "title": "A story about my father",
    "section": "",
    "text": "My father passed away a few years ago. I don’t even remember exactly when. I think it was in 2013 or 2014. We were not close when he died, and I don’t remember ever being close. To my memory, he was mostly away since I was around 7, and he left home for good when I was 12 or 13. Not having a father made a lot of things harder in my life, which I imagine made me more capable in some respects."
  },
  {
    "objectID": "posts/2019-02-04-a-story-about-my-father.html#what-i-remember-about-my-father",
    "href": "posts/2019-02-04-a-story-about-my-father.html#what-i-remember-about-my-father",
    "title": "A story about my father",
    "section": "What I remember about my father",
    "text": "What I remember about my father\nThere are a few things I remember about my father.\n\nI remember that once he carried me from his bed to my bed. I was about 6 and I was amazed that somebody could carry me. I don’t remember being carried by anyone else after that. I think about this when I carry my kids.\nI remember when he was supposed to give me breakfast (a task that always belonged to my mother) and that he wasn’t much into it. I remember complaining about the way he was slicing the bread (and smushing it) and that it made him upset. To this day, I am careful when I’m slicing bread for my kids.\nI remember that once he gave me a videogame for Christmas that was not what I wanted. He gave me the Intellivision game Snafu, I don’t remember exactly what I wanted. He was again very upset. Thinking back, I should probably have disguised my disappointment better, but I was 7. I remember that I liked the game after playing it and profusely apologized, but he didn’t accept my apologies."
  },
  {
    "objectID": "posts/2019-02-04-a-story-about-my-father.html#being-a-good-detective-is-not-always-great",
    "href": "posts/2019-02-04-a-story-about-my-father.html#being-a-good-detective-is-not-always-great",
    "title": "A story about my father",
    "section": "Being a good detective is not always great",
    "text": "Being a good detective is not always great\nMy father was a philanderer. I also remember that he would go out very frequently and leave me with my mother, so I didn’t see him very much. At some point, he got really attached to another woman (he would end up marrying her later) who had really dark, long hair. He must have told my mother that the relationship had ended, and a few months later, he traveled to a fishing trip with his friends. When he came back, I remember finding a long black hair when helping him unpack my suitcase. My mother and him had a huge fight about it. I was about 12 and didn’t immediately understand why that hair was such a big deal."
  },
  {
    "objectID": "posts/2019-02-04-a-story-about-my-father.html#i-still-do-things-differently",
    "href": "posts/2019-02-04-a-story-about-my-father.html#i-still-do-things-differently",
    "title": "A story about my father",
    "section": "I still do things differently",
    "text": "I still do things differently\nI make many decisions in which the main goal is simply to be different from my father. I don’t eat his favorite foods, for example. I’m careful when slicing bread for my kids, and I carry them around more than most dads. But I still can’t control “being a good detective”. Sometimes, when I find something that I shouldn’t reveal, I still can’t control myself. Hopefully, my kids will feel differently about me when the time comes. I’m working hard on it."
  },
  {
    "objectID": "posts/2019-02-12-the-day-somebody-stole-my-work.html",
    "href": "posts/2019-02-12-the-day-somebody-stole-my-work.html",
    "title": "The day somebody “stole” my work",
    "section": "",
    "text": "Part of the work of data science is to create machines that people can use to solve problems. Sometimes these machines can be re-used for other problems with little need for reconfiguration. Sometimes the people reusing one of the machines a data scientist creates will do so without letting the data scientist know until there’s a problem.\nTo protect the innocent, some details of the stories below have been changed."
  },
  {
    "objectID": "posts/2019-02-12-the-day-somebody-stole-my-work.html#the-chatbot",
    "href": "posts/2019-02-12-the-day-somebody-stole-my-work.html#the-chatbot",
    "title": "The day somebody “stole” my work",
    "section": "The chatbot",
    "text": "The chatbot\nYears ago, I wrote a chatbot to automate some really mundane question-answering for my team. Although it was not something extremely high-tech, it was the exact right tool for the job. Most chatbots I’ve seen are “Rube Goldberg machines”, like the “Self Operating Napkin” below. My chatbot really cut down on mundane manual work, and therefore was really successful.\n\n\n\nRube Goldberg Self-Operating Napkin\n\n\nBecause of its success, my chatbot got a lot of use and it started to be shown to customers as an example of our usage of technology. “Some guy in some place”, let’s call him G, became a specialist in demonstrating the chatbot I wrote, and he would do so very well.\nAfter a while, somebody had a question about my chatbot in an internal distribution list. The question went like “Can that chatbot do X?”. I answered that “No, it can’t do that - that was something that I had thought about but ended up never implementing it”. In a monumental display of corporate civility, someone replied over me saying: “You should ask G, as he is the creator of that chatbot”. I actually declined to further the discussion, and to my knowledge everybody on that thread still thinks that G is the creator of the chatbot.\n\n\n\nchatbot\n\n\nThat made me realize that G was not putting a lot of effort in attributing my work."
  },
  {
    "objectID": "posts/2019-02-12-the-day-somebody-stole-my-work.html#the-recommendation-system",
    "href": "posts/2019-02-12-the-day-somebody-stole-my-work.html#the-recommendation-system",
    "title": "The day somebody “stole” my work",
    "section": "The recommendation system",
    "text": "The recommendation system\nAt some point I wrote a recommendation system to solve a specific problem, let’s call it the “X Recommender”. It was not particularly well written but it did the job: it recommended X with a decent precision. However, by the time I was done, the client (let’s call them C) that needed that problem solved was moved out. I made a note to improve on that system at some point and stashed it.\nA few years later, someone wanted to be mentored by me, and they happened to be a specialist in X. So we started building models around X, and at some point we revived the “X Recommender” to make it better.\n\n\n\nRecommendation System\n\n\nAt this point, people from C came back asking for an “X Recommender”, and they also wanted to learn how to build one. I told them how to build one, step-by-step. They did not succeed in building it for production, but built it enough for a presentation and scheduled time for us to work together to make it ready for production.\nMeanwhile, I presented the very old “X recommender” to a group. A member of the group saw the similarities and asked me why I was taking credit for the work that C had done.\nThat made me realize that C was not putting a lot of effort in attributing my work."
  },
  {
    "objectID": "posts/2019-02-12-the-day-somebody-stole-my-work.html#the-consequences-of-not-having-the-work-attributed",
    "href": "posts/2019-02-12-the-day-somebody-stole-my-work.html#the-consequences-of-not-having-the-work-attributed",
    "title": "The day somebody “stole” my work",
    "section": "The consequences of not having the work attributed",
    "text": "The consequences of not having the work attributed\nIt’s not like I’m not recognized for my work, quite the opposite. I think the downside is more “societal”, or “corporate-wide”. In some sense, this is like a signalling problem in which a non-expert in a particular work product signals that they’re an expert. If the signal is misinterpreted, problems that the expert could solve may end up being directed to the non-expert, and that may result in solvable problems not being solved.\nThere are upsides, however. One upside is that a lot of trivial problems and issues gets filtered from the expert and solved by the non-expert instead. Another upside is that the non-expert will likely get better over time, if only to maintain the façade. Presumably the gain in skills from the non-expert will be faster than for the expert, so overall the skills for the company overall will increase."
  },
  {
    "objectID": "posts/2019-02-12-the-day-somebody-stole-my-work.html#i-actually-dont-have-an-answer",
    "href": "posts/2019-02-12-the-day-somebody-stole-my-work.html#i-actually-dont-have-an-answer",
    "title": "The day somebody “stole” my work",
    "section": "I actually don’t have an answer",
    "text": "I actually don’t have an answer\nI’m not sure how to make this better. I have received feedback that I don’t “sell” my work enough, but it’s a complicated equation…\n\n\n\nComplicated math\n\n\nThe problem is that selling the work takes time, and that will sometimes result in less time to do high-quality data science work. It may also result in wider than optimal adoption, which will require extra support, also detracting to high-quality data science work. Perhaps it pays better, but higher-quality also generally pays better than lower-quality, so it’s not that simple.\nOf course, I wish my work was such high-quality that this was the real problem. The likely explanation is that I rather work with models than sell them."
  },
  {
    "objectID": "posts/2018-08-23-yo-stroopwafel.html",
    "href": "posts/2018-08-23-yo-stroopwafel.html",
    "title": "Yo, Stroop!",
    "section": "",
    "text": "YoStroop is a bot that allows members of a slack channel to give rewards to each other, usually as a way of saying “thank you” for help and valuable contributions. It was developed using Python, Azure Functions v2.0 and Cosmos DB."
  },
  {
    "objectID": "posts/2018-08-23-yo-stroopwafel.html#motivation",
    "href": "posts/2018-08-23-yo-stroopwafel.html#motivation",
    "title": "Yo, Stroop!",
    "section": "Motivation",
    "text": "Motivation\nOne of the Data Science Slack channels I’m a member of tried several different tools to keep track of the valuable contributions of its members. Some of the tools that we used to to track contributions became quite expensive and we started looking for an alternative. Since most people in the channel can program in R and Python, I thought we might develop our own Slack bot. Developing a Slack Bot is really straightforward. You essentially need a HTTP server that will receive JSONs and call Web APIs. I wanted to create a Slack bot that was:\n\nWritten in a language that most Data Scientists are familiar with (e.g. Python or R)\nVirtually free to run\nServerless\nBased on very well known technologies (e.g., pyodbc for databases)\nEasy to modify from GitHub with no hassle\nDeveloped and maintained in any major platform: Windows, macOS or Linux\n\nI have failed to achieve almost all of the goals above, but maybe will achieve most in the near future."
  },
  {
    "objectID": "posts/2018-08-23-yo-stroopwafel.html#azure-functions-v2",
    "href": "posts/2018-08-23-yo-stroopwafel.html#azure-functions-v2",
    "title": "Yo, Stroop!",
    "section": "Azure Functions v2",
    "text": "Azure Functions v2\nMicrosoft recently released a Python worker for the Azure Functions runtime v2. As of this writing (2018-08-23), both Azure Functions v2 and the python worker are in “preview mode”. The Azure Functions v1 runtime supports development and hosting only in the portal or on Windows. The v2 runtime runs on .NET Core, and therefore can run on Windows, macOS and Linux. Microsoft provides a set of tools based on Node.JS and .NET Core that make it easy to develop in any platform. That takes care of objectives #1, #3 and #6. Although knocking off three objectives simply by selecting Azure Functions v2 sounded very promising to begin with, that’s as many objectives as I could achieve.\n\nLinux host\nRegardless of the platform you develop on, Python function apps need to be hosted on Azure Functions on Linux. Running on Linux has a couple downsides:\n\nYou have to access the Azure Portal using a secret code to enable Azure to offer Linux hosts\nAs of this writing (2018-08-23), Linux hosts can only be dedicated. Azure Functions can normally run in two modes: “Consumption” and “Dedicated”. Consumption is virtually free, but the “dedicated” mode costs approximately $35 per month for the cheapest version. According to the README the “Consumption” mode is upcoming, and I have a small allowance due to my MSDN subscription, so I caved.\n\nAnd there goes objective #2. But it seems temporary, so we’re probably good.\n\n\nDeveloping a Slack Bot\nA Slack bot is essentially an HTTP server that will receive JSONs and perform actions through HTTP requests. Azure Functions have an HTTP Trigger template, so this takes care of most of the plumbing. I created a Slack app using the instructions here. This bot subscribes to events, which are sent to an URL. I’ve created the HTTP Server that provides that URL using the instructions on how to create a function using the command line interface, and creating a sample based on the HttpTrigger.\nSpecifically, my Azure Function receives JSONs for each message, parses them to see if they have a :stroopwafel:, and if they do, parses the mentions (@user) in the message. For each mention, we log a :stroopwafel: gift in a database, from which we can build reports at the end of a period.\n\n\nPyODBC\nThe original idea was tha whenever a message mentioned a :stroopwafel:, the bot would use pyodbc to save the message in a database. Databases are usually not free, but Azure offers SQL Server for as low as $5 per month, and that seemed acceptable. The problem is that pyodbc does not have a linux wheel, and currently Azure Functions for Linux requires wheels. There’s a way of compiling the package on the fly, but it currently has a bug. And I don’t know how long it will take to fix that bug.\nI searched for alternatives to PyODBC, and the best alternative was to use Cosmos DB, since its Python package pydocumentdb has a wheel. The downside is that the cheapest Cosmos DB plan as of this writing costs $25. And now we’re far away from objective #2, and we’re also not doing great with objective #4. However, I found Cosmos DB a pleasure to use.\n\n\nGitHub\nOnce I was done with my development, which Azure Functions Core Tools made really convenient by setting a local test server whenever I run func host start, I pushed my repository to GitHub and configured the Azure Function app to consume it using Continuous Deployment, which is something that you can set up in Azure with just a few clicks. However, it doesn’t yet work (sad trombone). Luckily, deploying without GitHub is not that bad. Again, Azure Functions Core Tools provides a command-line interface utility that pushes your application to the server: func azure functionapp publish <app-name>, and you’re done in a couple of minutes. But I also don’t fulfill objective #5."
  },
  {
    "objectID": "posts/2018-08-23-yo-stroopwafel.html#verdict",
    "href": "posts/2018-08-23-yo-stroopwafel.html#verdict",
    "title": "Yo, Stroop!",
    "section": "Verdict",
    "text": "Verdict\nAlthough I was so far unable to fulfill half of my initial objectives, it looks like I’ll get there soon. There’s something magical about Azure Functions. It does a lot of the hard work for you, and if you’re familiar with one of the many languages available, you can deploy your program to production really fast. I had never developed a Slack bot before, and I’ve started the bot during a sleepless night and finished it in an afternoon. All in, it took me less than two days of work to get an app to production. There are still some kinks to be worked out but Azure Functions v2 has already proved to be useful to me.\nI wrote about Azure Functions before, specifically about using Python in Azure Functions v1. Azure Functions can be very useful for Data Scientists: it’s an easy way to deploy a model to production, perform scheduled maintenance, update datasets on a schedule, etc. With v2, Python became a first class player. I’m sure I’ll use it a lot."
  },
  {
    "objectID": "posts/2019-02-13-why-you-should-blog-if-you-are-a-data-scientist.html",
    "href": "posts/2019-02-13-why-you-should-blog-if-you-are-a-data-scientist.html",
    "title": "Why you should blog if you are a data scientist",
    "section": "",
    "text": "A while back, David Robinson offered the following advice on Twitter:\nHe also followed it up with an excellent blog post aimed at aspiring data scientists. I think that the most important idea from that post is actually from a presentation he gave. Here’s the key idea, as reported by Amelia McNamara:\nIn summary, things that you keep to yourself have very little value, and things that you share with the world have a lot of value. I think that’s a very cool idea for the Economics of information. For an extreme case, imagine you have a trade secret that allows you to solve a specific class of problems faster than others. In this case, it’s understandable that you want to keep the trade secret to yourself. But it makes sense to advertise to the world that you can quickly solve some kinds problems faster than others. That makes your trade secret more valuable."
  },
  {
    "objectID": "posts/2019-02-13-why-you-should-blog-if-you-are-a-data-scientist.html#the-discipline-of-writing-about-what-youre-doing",
    "href": "posts/2019-02-13-why-you-should-blog-if-you-are-a-data-scientist.html#the-discipline-of-writing-about-what-youre-doing",
    "title": "Why you should blog if you are a data scientist",
    "section": "The discipline of writing about what you’re doing",
    "text": "The discipline of writing about what you’re doing\nWhen I was a wee little kid and had just entered college, one of my first classes was “Physics Lab”, and the first class of that was to measure “gravity”, more specifically, the gravitational acceleration . Of course, mathy Computer Science studies that we were, we all knew that the gravitational acceleration would be \\(g \\approx 9.8 m/s^2\\).\nMeasuring it, however, it’s not very easy. First, remember that this is in the early 90s in Brazil, so digital cameras were very rare. Part of the problem is that 9.8 meters (approximately 30 feet) is quite high, so if we wanted our experiment to take around 1s, we would need a big ladder. Or, as it happened, we’d need to run an experiment that took less than a second for each run. In a class with 25 students, that was the preferred route.\nWe set up a vertical track attached to a device that would spark every 1/60 of a second, and we attached grid paper to the track. This is called a Behr free fall apparatus.\nWe would release the device from the top of the track, and the sparks would mark the grid paper. We would then manually measure the distances between the sparks and the differences between the distances would tell us the acceleration. In theory. Again, remember this is the early 90s, so there’s no Windows 95 or Excel easily available, there were several steps that were prone to error.\nThe desired outcome was not only to calculate the acceleration due to gravity, but also to generate a lab report."
  },
  {
    "objectID": "posts/2019-02-13-why-you-should-blog-if-you-are-a-data-scientist.html#writing-the-lab-report",
    "href": "posts/2019-02-13-why-you-should-blog-if-you-are-a-data-scientist.html#writing-the-lab-report",
    "title": "Why you should blog if you are a data scientist",
    "section": "Writing the lab report",
    "text": "Writing the lab report\nIt was a simple experiment, but there was a twist. Another class of 25 students would have to replicate the experiment following the lab report from the first 25. Oh boy. We quickly found out that the best lab reports were the ones in which the experimenter would document the experiment as they were executing the experiment. Another thing that worked was going through the experiment more than once. What did not work was to perform the experiment and then go to where the computers were to type up a report from memory.\nI thought that was very insightful, shortly afterwards, we would learn about Donald Knuth’s proposed paradigm of literate programming, which lives on in Jupyter Notebooks and R Markdown and that is very popular in data science today. Behind it is the same concept of explanations interspersed with technical work to produce something that is reproducible and easy to understand."
  },
  {
    "objectID": "posts/2019-02-13-why-you-should-blog-if-you-are-a-data-scientist.html#blogging-helps-you-train-to-write-about-your-work",
    "href": "posts/2019-02-13-why-you-should-blog-if-you-are-a-data-scientist.html#blogging-helps-you-train-to-write-about-your-work",
    "title": "Why you should blog if you are a data scientist",
    "section": "Blogging helps you train to write about your work",
    "text": "Blogging helps you train to write about your work\nBesides the excellent reasons offered by David Robinson for aspiring data scientists to blog, I think experienced data scientists can also benefit from blogging. David argues that aspiring data scientists should blog for technical practice, to build a portfolio, and to get feedback. Experienced data scientists might benefit for similar reasons, but not exactly the same.\nSimilar to “practicing”, blogging about technical topics helps you learn how to communicate better about them, and also helps you learn how to write better research reports. Similar to “portfolio”, it signals a data scientist’s body of work. For an early-in-career data scientist, it may be more showing that you can do analysis, but for an experienced data scientist is more about what type of analysis you like to do. Finally, I think that “feedback” also works differently for early-in-career and experienced data scientists. I’m not sure random strangers in the internet will sweep in and correct your posts, but you can find out what people are reading and sharing. You may be surprised when you learn what parts of your work people find interesting."
  },
  {
    "objectID": "posts/2018-06-25-centralized-vs-distributed-data-teams.html",
    "href": "posts/2018-06-25-centralized-vs-distributed-data-teams.html",
    "title": "Centralized vs. Distributed Data Science Teams",
    "section": "",
    "text": "In a recent DataFramed podcast, Hugo Bowne-Anderson and Jonathan Nolis discuss whether data scientist teams should be centralized or distributed. I have my own opinions about that question."
  },
  {
    "objectID": "posts/2018-06-25-centralized-vs-distributed-data-teams.html#should-data-science-teams-be-centralized-or-distributed",
    "href": "posts/2018-06-25-centralized-vs-distributed-data-teams.html#should-data-science-teams-be-centralized-or-distributed",
    "title": "Centralized vs. Distributed Data Science Teams",
    "section": "Should data science teams be centralized or distributed?",
    "text": "Should data science teams be centralized or distributed?\nI was happy to find that Jonathan Nolis has the same opinion as I do: that a data science team works best when distributed into client areas. In addition, we agree that if you distribute your data science team among several different client areas, you have to find a way of keeping the data scientists feeling as if they’re part of a cohesive group. And that’s hard.\nFor me, a key part of the argument to distribute data scientists in client areas relies on Drew Conway’s (ancient) Data Science Venn diagram.\n\n\n\nData Science Venn Diagram\n\n\nAssigning data scientists to a client area will help build the “Substantive Expertise” part of the diagram. A common trap here is “Well, we can do the same if we keep the data scientists centralized and give them specializations that match the client areas.”\nAlthough I think that can work, I worry about what happens when data scientists have downtime.\n\nIncentives for client areas and centralized data science teams\nIn my observations (low sample size), the client areas usually have a better sense of ROI1 - if they invest resources on something, they expect returns. On the other hand, most centralized data science teams I’ve seen have two problems: data scientists hired without a clear purpose and an incentive to overengineering: a current example is to use Deep Learning where a regression would do.\n\n\n\nOverengineering\n\n\nIt may be simply that client areas are more mature and battle worn than centralized data science groups, at least in my experience. I think that if a data scientist is assigned to a client area and there’s downtime, they’ll put the data scientist to work in business intelligence or decision analysis. On the other hand, if the data scientist is assigned to a centralized data science group, they’ll be pulled into a project away from their “client area specialization” or will find a way to work in something novel that’s cool (as of 2018, autoencoders, GANs or something with “Bayesian” in its name).\nAnd here the ROI appears again: as the data scientist works through the BI project, they’ll learn more about the data and more about the business and its data, and that investment has a large likelihood of return in their next project in the same group. Whereas if they spend this time learning a new technique, the return is uncertain.\n\n\nBut wait, if you do that, data scientists will never learn something new\nHere’s where the hard part of keeping a data scientist community pays off. That community may be weekly “Lunch & Learns” if the number of data scientists is small, or quarterly conferences if the number of data scientists is very big and your company is very, very rich.\nIf you don’t forcefully push towards having such a community, it’s very likely that data scientists will reinvent the wheel over and over again, and if their time is as precious as their high salary indicates, that will be a waste of money. For example, if you have 5 data scientists It’s better to spend $50k on travel to bring them together a few times a year than to have one of them wasting three months reinventing something that another of them already done.\n\n\nWhere I disagree with Jonathan Nolis\nIn the podcast, Jonathan suggests that the data scientists should be assigned to client areas and report “solid line” to a Chief Data Scientist, while reporting “dotted line” to the client areas. I would do the opposite. However, listening to the podcast, it seems that Jonathan didn’t have a strong opinion that one is better than the other, and again in agreement, neither do I.\nOne big advantage of having a “solid line” group of data scientists is career advancement. It’s easier to compare data scientists with other data scientists. Leaving them in the client areas will require to compare them with other members of the client area, and they may be seen as wizards (and overvalued) or “too hard to understand”, and be undervalued.\nOne disadvantage is estimating the value of the data scientist to the client area. Data scientists that are more creative in their value calculations or less scrupulous are probably going to get ahead. I’ve lost count of the times that a data science project saved more revenue than the customer ever had.\n\n\nSo, what do I do?\nHonestly, if you’re starting a data science team, your best bet is to hire someone like Jonathan or Drew Conway to plan the team for you. You’re going to spend a lot of money on Data Science, and it’s important to make sure that it will be well spent.\nIf you’re on a budget and all you can invest is the time to read some blog posts, well… you probably don’t have enough money to hire data scientists. But if you do, I recommend assigning them to the client areas and keeping them there. Even when they’re not doing modeling or machine learning, they’re learning more about the business and its data."
  },
  {
    "objectID": "posts/2018-07-08-praising-conway-s-data-science-venn-diagram.html",
    "href": "posts/2018-07-08-praising-conway-s-data-science-venn-diagram.html",
    "title": "Praising Conway’s Data Science Venn Diagram",
    "section": "",
    "text": "Drew Conway’s Data Science Venn Diagram is perhaps the most well-known description of data science.\nDrew published it more than five years ago. Since then, a ridiculous number of Venn diagrams have been created, one worse than the other.\nFive years later, people still struggle with the definition of Data Science. A recent famous example was Lyft changing the titles of their Data Scientists, Data Analysts and Research Scientists."
  },
  {
    "objectID": "posts/2018-07-08-praising-conway-s-data-science-venn-diagram.html#should-we-care",
    "href": "posts/2018-07-08-praising-conway-s-data-science-venn-diagram.html#should-we-care",
    "title": "Praising Conway’s Data Science Venn Diagram",
    "section": "Should we care?",
    "text": "Should we care?\nThere’s a lot of information asymmetry in the Data Science market, the position being so new, and “Michael Spence signaling” matters. Having a “Data Scientist” title from a reputable company is an important signal. For example, a medium-sized company might not be aware that Lyft recently changed their data analysts title to “Data Scientist”\nPart of the problem is that the “Data Scientist” title is itself poorly defined, as shown by the litany of Venn and not-Venn diagrams listed above. A short, non-comprehensive list of alternative titles for Data Scientists I’ve met in the last month:\n\nTableau/Power BI Report Designer\nMachine Learning Researcher\nPython Programmer who knows how to call scikit-learn functions\n\nUsing the same name for vastly different abilities makes matching between employers and employees difficult. In addition, it may make career progression difficult. For example, job responsibilities and objectives are likely to be related to a company’s interpretation of the title of “Data Scientist”. Employees in that company may be “Data Scientists” under other interpretations, and may be really good, but may have their performance evaluated under different guidelines: “I know your ML model saved $1B for our company, but the color scheme of your Tableau reports is horrible!” As per the (not really) Albert Einstein’s quote “(…) if you judge a fish by its ability to climb a tree, it will live its whole life believing that it is stupid.”"
  },
  {
    "objectID": "posts/2018-07-08-praising-conway-s-data-science-venn-diagram.html#conways-venn-to-the-rescue",
    "href": "posts/2018-07-08-praising-conway-s-data-science-venn-diagram.html#conways-venn-to-the-rescue",
    "title": "Praising Conway’s Data Science Venn Diagram",
    "section": "Conway’s Venn to the rescue!",
    "text": "Conway’s Venn to the rescue!\nHere’s where I think Conway’s Venn diagram comes to the rescue in a useful way. We could rule out the Tableau Designer if they lack programming or statistical skills. We could rule out the ML Researcher if they don’t have domain expertise. We could rule out the Python programmer if they don’t know Math and/or have domain expertise. Perhaps this makes Data Scientists hard to find (it would justify their salary)!\nDepending what the company you work for was before the Data Science profession started, a lot of people from different jobs may have re-branded themselves as data scientists. I think Conway’s diagram is helpful as it points to where people should focus their development in order to be more effective as data scientists. For example, former Economists most likely need more hacking skills, former computer scientists need more Statistics and domain expertise, and former management/technical consultants likely need more Statistics.\nIt can also serve as a check for inclusiveness. For example, a team of former machine learning engineers is struggling to deliver meaningful projects because they lack domain expertise. However, they can’t hire domain experts because the domain experts keep failing the machine learning interviews. Comically, the machine learning engineers think that domain expertise “is easy”.\n\n\n\nEverything you don’t know is easy\n\n\nThe most useful part of Conway’s diagram for me is that I’ve time and again observed projects that don’t have representatives that can communicate well with each other in all three areas to fail. Conway’s diagram even makes it easy to show how the project failed (“nobody with domain expertise”, “missing knowledge of statistics”). Having a single person that can do all three is difficult, but it certainly solves the communication problem. That being hard or impossible, having specialists that can communicate with each other will usually be sufficient."
  },
  {
    "objectID": "posts/2018-07-08-praising-conway-s-data-science-venn-diagram.html#but-conways-venn-is-missing-something",
    "href": "posts/2018-07-08-praising-conway-s-data-science-venn-diagram.html#but-conways-venn-is-missing-something",
    "title": "Praising Conway’s Data Science Venn Diagram",
    "section": "But Conway’s Venn is missing something!!!",
    "text": "But Conway’s Venn is missing something!!!\nTrue to the famous Box aphorism that “all models are wrong but some are useful”, Conway’s Venn is not perfect. In my experience, one of the things that I usually miss is “intensity”. Someone that knows how to write a “Hello World!”, knows how to take averages, and know how to calculate compound interest has knowledge in all three areas of the Venn diagram, but is probably not the ideal person to lead a data science project in Finance. Conway’s is best used to tell you what’s missing for career development, for a team, for a project, but it will not solve all problems.\nIt will, however, solve many problems that matter, and will provide useful information very elegantly. You can’t ask much more from a model."
  },
  {
    "objectID": "posts/2017-09-11-hurricane-irma.html",
    "href": "posts/2017-09-11-hurricane-irma.html",
    "title": "Hurricane Irma",
    "section": "",
    "text": "Hurricane Irma happened when I was out on a business trip, and my family was in Parkland, FL. Forecasts went from “maybe it will hit you” to “if you stay you’ll surely die” to “I think it’s not going to hit you anymore” to “Phew, it will miss” in four days.\nWe did a lot of preparation using conditional probabilities. In our assessment, it would be very unlikely for the hurricane to hit both the East and West coast. So we decided to move the family from Parkland, FL to Tampa (back then, the hurricane was forecasted to hit the East coast), but to book hotels in both Pensacola (Florida panhandle) and Jacksonville, FL while we waited for new forecasts.\nAs days went by, the forecasts moved further and further West. On Saturday 2 AM, the forecast was that the hurricane would hit Tampa as a Category 4, so staying there was too risky. And then we had to decide between moving to the Florida panhandle, moving to Jacksonville and incredibly, going back to Parkland, near Fort Lauderdale.\nThe uncertainty seemed higher for places that were north of Tampa: it was hard to tell whether the hurricane would turn either way in 3 days. On the other hand, the forecasts are better in close proximity to the hurricane, and it seemed pretty certain that the hurricane would miss the Parkland/Boca Raton area. So we made the somewhat gutsy call that my wife would come back to Parkland with the kids.\nThere’s surely an element of luck to all of this, but the outcome was positive. We had very minor damage in our house: a couple plants and a downspout. I think the total amount of damage is going to be less than $100. It’s hard to be precise because there may be damage that is hard to see, like on the roof. We did not lose power, internet or even satellite TV. The hardest part was the stress.\nOn Monday morning the winds were back to normal. There’s still a lot of debris around the county. I’m supposed to fly back home tomorrow, if the airport opens, and hopefully I’ll be able to get a ride home. This was more of a suspense than a horror movie. Many people were not as lucky as we were and we’ll now see how we can help them."
  },
  {
    "objectID": "posts/jekyll-to-quarto/index.html",
    "href": "posts/jekyll-to-quarto/index.html",
    "title": "Converting my blog from Jekyll to Quarto",
    "section": "",
    "text": "What I wanted in a blog platform\nFor a long time, I had an unfulfilled wish list for a blog platform:\n\nCan write posts in Markdown\nEasy to deploy to Github\nCan script in Python\n\nA large number of platforms fulfills the first two items, but I struggled to find one that fulfilled the last item. I finally found Pelican, but I found it too clunky and started to worry that it was not going to be maintained for long. Therefore, I oscillated between Jekyll and Hugo (using blogdown) for a long time. I liked blogdown a lot, but whenever I’m switching a lot between R and Python, my brain gets really unhappy.\n\n\nQuarto has everything, and more\nRecently, I found Quarto that ticks all my boxes and solves a lot of other problems. My feeling when using Quarto must be similar to the feeling of Python programmers when they started using Jupyter (because they never used RStudio) or similar to what I felt when, as an analyst, started using RStudio. Things just work.\nWhen I was initially switching between Jekyll and Hugo, one of the most annoying things was Jekyll preferred posts as files, while blogdown preferred posts as directories. RStudio even provides a function bundle_site() to convert files to directories. Now, when I’m converting to Quarto, I thought I needed to write such a function in Python.\nAgain, it turns out that “things just work”. Quarto accepts both directores and files as posts and rendered everything correctly. So I’m going to draw the “Quarto Spiral” instead, just to celebrate!\n\n\nExample from Hello, Quarto!\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/2019-01-29-installing-arch-linux.html",
    "href": "posts/2019-01-29-installing-arch-linux.html",
    "title": "Installing Arch Linux",
    "section": "",
    "text": "Even though I work at Microsoft and really love Windows, I’ve recently installed Arch Linux as a dual boot.\nA lot of data scientists and software developers I know prefer OS X, and Windows doesn’t get that much love. Although many people provide many reasons not to use Windows, I was always able to work around whatever those issues were. Sometimes this required Cygwin, Git Bash, and more recently, WSL (the Windows Subsystem for Linux). I can’t remember any software that I was ever unable to use in Windows: LaTeX, git, Inkscape… everything I need had a decent Windows version, even if I had to compile it from source.\nWeirdly, because I can do so much with Windows, I find that I take too many shortcuts. The most damaging shortcuts are the ones that damage reproducibility, especially when creating documentation for my projects. It’s too easy to copy and paste into PowerPoint and be done with the communications for a talk. Some of my talks, especially about my most successful product, have many similar versions with one or two slides changed. Someone asks me for a slightly shorter or slightly longer talk and I suddenly have a new file with one or two extra (or hidden) slides that I may need to maintain. It’s not lost on me that this is a lack of discipline on my part, but if you were to invite me to a brigadeiro store, it would be hypocritical of you to accuse me of having no self-control."
  },
  {
    "objectID": "posts/2019-01-29-installing-arch-linux.html#why-arch",
    "href": "posts/2019-01-29-installing-arch-linux.html#why-arch",
    "title": "Installing Arch Linux",
    "section": "Why Arch",
    "text": "Why Arch\nI first got into contact with Arch through a German friend back in 2010. When trying to install it, I quickly learned that Arch requires you to understand what you’re doing, and especially, that if you don’t understand what you’re doing, things break. Over time, I found that although some things have a steep learning curve, the return on the time investment was very positive. I also tried Ubuntu, Fedora and Red Hat, and although I could get productive more quickly, keeping them configured to my taste later was harder - when something broke it took me longer to understand what it was.\nThere are several things that still don’t work well in Linux. One of the most vexing is having a high-DPI display. It’s somewhat hard to configure it. It works well in KDE, but KDE has tearing issues. It doesn’t work too well in GNOME unless you use Wayland, which brings a host of different issues. You can make everything a lot worse by using the NVidia proprietary driver with Wayland, but it’s needed if you want to use the GPU efficiently.\nSo although I managed to configure my Arch installation to use GNOME under Wayland and use the NVidia proprietary driver, it’s very unstable. Native applications tend to run, but many applications crash in comical ways: resizing Google Chrome under Wayland is a sure way to cause a crash, for example."
  },
  {
    "objectID": "posts/2017-11-17-lenovo-p51-the-data-scientist-laptop.html",
    "href": "posts/2017-11-17-lenovo-p51-the-data-scientist-laptop.html",
    "title": "Lenovo P51 - the Data Scientist’s laptop",
    "section": "",
    "text": "The Microsoft Surface Pro 3 was the right computer for me at the time I got it, but as time went by my position changed substantially, and it ceased to be appropriate for me.\nWhen I got the Surface, the core of my work consisted in traveling, having conference calls and light emailing and spreadsheet work. As time went by, my work went back to creating analytics, creating presentations about analytics, and traveling. There is where the screen of the Surface becomes a liability at 12 inches. I wanted something bigger.\nAnd boy, is the P51 bigger. It has a 15-inch screen and a power brick that alone is twice as heavy as the Surface Pro. It’s big enough to have a keyboard that comes with a numeric keypad. Since I still have to travel a lot, I chose the P51 over the beefier 17-inch P71. The P71 won’t even fit in my somewhat large Osprey backpack. The P51 barely fits.\nDue to some standardization constraints, my P51 is the 1920x1080 version with touch screen. It has 32 Gb of RAM and a 500 Gb SSD. It has the i7 7820HQ running at 2.9 GHz and a Quadro M2200 GPU. If I had the option, I’d choose the 4K screen and the Xeon processor, but I did not have that option.\nThe battery life is surprisingly good. For the kind of things that I do in planes (writing blog posts, editing presentations, light programming), the battery seems to last well over 8h without Wi-Fi and about 7h with Wi-Fi. I have yet to run out of battery in a flight. I have read that the earlier version of the P51, the P50, would trip the circuit breaker of a plane if you tried to plug it in. I have not attempted to plug in the P51 yet.\nThe P51 is usable in the Premium Economy seats and, of course, in business and first class seats, but I have the impression that it would not fit well in the economy class of carriers that have a very small distance between seats (like American Airlines domestic flights), especially if the person in front of you reclines their seat. It works well in exit seats.\nOne gripe I had about the Surface Pro 3 was that I always had to travel with a USB hub, a mouse, and a couple mini-DisplayPort adapters. The P51 comes with a mechanical keyboard, the beloved TrackPoint, a good TouchPad, 4 USB ports, a Thunderbolt 3 port, an Ethernet port, an HDMI port, and, to my surprise, a mini-DisplayPort, so I get to keep all the adapters I had for the Surface.\nI’ve also considered the P51-S. The slimmer version trades off some power (it goes from quad-core to dual-core) for a thinner, lighter frame. I think the P51-S would also have been a solid choice, but I think it would force me to do most of my work from a workstation and/or a server that I’d need to remote into frequently, adding to the stress of versioning and synchronizing. Somewhat counter-intuitively, I chose the heavier computer because I travel a lot, as it has enough power for me to do the vast majority of my work on it and use a server or workstation just for the real hardcore stuff.\nIn summary, the Lenovo P51 is a great computer for the traveling data scientist. It’s powerful enough to be used as your primary machine. For me, it’s near the top limit of a computer that is comfortable to travel with, but still within that limit. I got used to it in no time and completely gave up my desktop workstation."
  },
  {
    "objectID": "posts/2018-02-20-the-three-types-of-gifts-in-buddhism.html",
    "href": "posts/2018-02-20-the-three-types-of-gifts-in-buddhism.html",
    "title": "The three types of gifts in Buddhism",
    "section": "",
    "text": "In Buddhism, there are three kinds of gifts. The first is the gift of material resources. The second is to help people rely on themselves, to offer them the technology and know-how to stand on their own feet. (…)\n\n\nThe third is the gift of non-fear. We are afraid of many things. We feel insecure, afraid of being alone, afraid of sickness and dying. To help people not be destroyed by their fears, we practice the third kind of gift-giving."
  },
  {
    "objectID": "linkedin-index.html",
    "href": "linkedin-index.html",
    "title": "Lucas A. Meyer’s LinkedIn Posts",
    "section": "",
    "text": "May 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/yo-stroopwafel.html",
    "href": "posts/yo-stroopwafel.html",
    "title": "Yo, Stroop!",
    "section": "",
    "text": "YoStroop is a bot that allows members of a slack channel to give rewards to each other, usually as a way of saying “thank you” for help and valuable contributions.\nIt was developed using Python, Azure Functions v2.0 and Cosmos DB."
  },
  {
    "objectID": "posts/yo-stroopwafel.html#motivation",
    "href": "posts/yo-stroopwafel.html#motivation",
    "title": "Yo, Stroop!",
    "section": "Motivation",
    "text": "Motivation\nOne of the Data Science Slack channels I’m a member of tried several different tools to keep track of the valuable contributions of its members. Some of the tools that we used to to track contributions became quite expensive and we started looking for an alternative. Since most people in the channel can program in R and Python, I thought we might develop our own Slack bot. Developing a Slack Bot is really straightforward. You essentially need a HTTP server that will receive JSONs and call Web APIs. I wanted to create a Slack bot that was:\n\nWritten in a language that most Data Scientists are familiar with (e.g. Python or R)\nVirtually free to run\nServerless\nBased on very well known technologies (e.g., pyodbc for databases)\nEasy to modify from GitHub with no hassle\nDeveloped and maintained in any major platform: Windows, macOS or Linux\n\nI have failed to achieve almost all of the goals above, but maybe will achieve most in the near future."
  },
  {
    "objectID": "posts/yo-stroopwafel.html#azure-functions-v2",
    "href": "posts/yo-stroopwafel.html#azure-functions-v2",
    "title": "Yo, Stroop!",
    "section": "Azure Functions v2",
    "text": "Azure Functions v2\nMicrosoft recently released a Python worker for the Azure Functions runtime v2. As of this writing (2018-08-23), both Azure Functions v2 and the python worker are in “preview mode”. The Azure Functions v1 runtime supports development and hosting only in the portal or on Windows. The v2 runtime runs on .NET Core, and therefore can run on Windows, macOS and Linux. Microsoft provides a set of tools based on Node.JS and .NET Core that make it easy to develop in any platform. That takes care of objectives #1, #3 and #6. Although knocking off three objectives simply by selecting Azure Functions v2 sounded very promising to begin with, that’s as many objectives as I could achieve.\n\nLinux host\nRegardless of the platform you develop on, Python function apps need to be hosted on Azure Functions on Linux. Running on Linux has a couple downsides:\n\nYou have to access the Azure Portal using a secret code to enable Azure to offer Linux hosts\nAs of this writing (2018-08-23), Linux hosts can only be dedicated. Azure Functions can normally run in two modes: “Consumption” and “Dedicated”. Consumption is virtually free, but the “dedicated” mode costs approximately $35 per month for the cheapest version. According to the README the “Consumption” mode is upcoming, and I have a small allowance due to my MSDN subscription, so I caved.\n\nAnd there goes objective #2. But it seems temporary, so we’re probably good.\n\n\nDeveloping a Slack Bot\nA Slack bot is essentially an HTTP server that will receive JSONs and perform actions through HTTP requests. Azure Functions have an HTTP Trigger template, so this takes care of most of the plumbing. I created a Slack app using the instructions here. This bot subscribes to events, which are sent to an URL. I’ve created the HTTP Server that provides that URL using the instructions on how to create a function using the command line interface, and creating a sample based on the HttpTrigger.\nSpecifically, my Azure Function receives JSONs for each message, parses them to see if they have a :stroopwafel:, and if they do, parses the mentions (@user) in the message. For each mention, we log a :stroopwafel: gift in a database, from which we can build reports at the end of a period.\n\n\nPyODBC\nThe original idea was tha whenever a message mentioned a :stroopwafel:, the bot would use pyodbc to save the message in a database. Databases are usually not free, but Azure offers SQL Server for as low as $5 per month, and that seemed acceptable. The problem is that pyodbc does not have a linux wheel, and currently Azure Functions for Linux requires wheels. There’s a way of compiling the package on the fly, but it currently has a bug. And I don’t know how long it will take to fix that bug.\nI searched for alternatives to PyODBC, and the best alternative was to use Cosmos DB, since its Python package pydocumentdb has a wheel. The downside is that the cheapest Cosmos DB plan as of this writing costs $25. And now we’re far away from objective #2, and we’re also not doing great with objective #4. However, I found Cosmos DB a pleasure to use.\n\n\nGitHub\nOnce I was done with my development, which Azure Functions Core Tools made really convenient by setting a local test server whenever I run func host start, I pushed my repository to GitHub and configured the Azure Function app to consume it using Continuous Deployment, which is something that you can set up in Azure with just a few clicks. However, it doesn’t yet work (sad trombone). Luckily, deploying without GitHub is not that bad. Again, Azure Functions Core Tools provides a command-line interface utility that pushes your application to the server: func azure functionapp publish <app-name>, and you’re done in a couple of minutes. But I also don’t fulfill objective #5."
  },
  {
    "objectID": "posts/yo-stroopwafel.html#verdict",
    "href": "posts/yo-stroopwafel.html#verdict",
    "title": "Yo, Stroop!",
    "section": "Verdict",
    "text": "Verdict\nAlthough I was so far unable to fulfill half of my initial objectives, it looks like I’ll get there soon. There’s something magical about Azure Functions. It does a lot of the hard work for you, and if you’re familiar with one of the many languages available, you can deploy your program to production really fast. I had never developed a Slack bot before, and I’ve started the bot during a sleepless night and finished it in an afternoon. All in, it took me less than two days of work to get an app to production. There are still some kinks to be worked out but Azure Functions v2 has already proved to be useful to me.\nI wrote about Azure Functions before, specifically about using Python in Azure Functions v1. Azure Functions can be very useful for Data Scientists: it’s an easy way to deploy a model to production, perform scheduled maintenance, update datasets on a schedule, etc. With v2, Python became a first class player. I’m sure I’ll use it a lot."
  },
  {
    "objectID": "posts/a-story-about-my-father.html",
    "href": "posts/a-story-about-my-father.html",
    "title": "A story about my father",
    "section": "",
    "text": "My father passed away a few years ago. I don’t even remember exactly when. I think it was in 2013 or 2014. We were not close when he died, and I don’t remember ever being close. To my memory, he was mostly away since I was around 7, and he left home for good when I was 12 or 13. Not having a father made a lot of things harder in my life, which I imagine made me more capable in some respects."
  },
  {
    "objectID": "posts/a-story-about-my-father.html#what-i-remember-about-my-father",
    "href": "posts/a-story-about-my-father.html#what-i-remember-about-my-father",
    "title": "A story about my father",
    "section": "What I remember about my father",
    "text": "What I remember about my father\nThere are a few things I remember about my father.\n\nI remember that once he carried me from his bed to my bed. I was about 6 and I was amazed that somebody could carry me. I don’t remember being carried by anyone else after that. I think about this when I carry my kids.\nI remember when he was supposed to give me breakfast (a task that always belonged to my mother) and that he wasn’t much into it. I remember complaining about the way he was slicing the bread (and smushing it) and that it made him upset. To this day, I am careful when I’m slicing bread for my kids.\nI remember that once he gave me a videogame for Christmas that was not what I wanted. He gave me the Intellivision game Snafu, I don’t remember exactly what I wanted. He was again very upset. Thinking back, I should probably have disguised my disappointment better, but I was 7. I remember that I liked the game after playing it and profusely apologized, but he didn’t accept my apologies."
  },
  {
    "objectID": "posts/a-story-about-my-father.html#being-a-good-detective-is-not-always-great",
    "href": "posts/a-story-about-my-father.html#being-a-good-detective-is-not-always-great",
    "title": "A story about my father",
    "section": "Being a good detective is not always great",
    "text": "Being a good detective is not always great\nMy father was a philanderer. I also remember that he would go out very frequently and leave me with my mother, so I didn’t see him very much. At some point, he got really attached to another woman (he would end up marrying her later) who had really dark, long hair. He must have told my mother that the relationship had ended, and a few months later, he traveled to a fishing trip with his friends. When he came back, I remember finding a long black hair when helping him unpack my suitcase. My mother and him had a huge fight about it. I was about 12 and didn’t immediately understand why that hair was such a big deal."
  },
  {
    "objectID": "posts/a-story-about-my-father.html#i-still-do-things-differently",
    "href": "posts/a-story-about-my-father.html#i-still-do-things-differently",
    "title": "A story about my father",
    "section": "I still do things differently",
    "text": "I still do things differently\nI make many decisions in which the main goal is simply to be different from my father. I don’t eat his favorite foods, for example. I’m careful when slicing bread for my kids, and I carry them around more than most dads. But I still can’t control “being a good detective”. Sometimes, when I find something that I shouldn’t reveal, I still can’t control myself. Hopefully, my kids will feel differently about me when the time comes. I’m working hard on it."
  },
  {
    "objectID": "posts/why-you-should-blog-if-you-are-a-data-scientist.html",
    "href": "posts/why-you-should-blog-if-you-are-a-data-scientist.html",
    "title": "Why you should blog if you are a data scientist",
    "section": "",
    "text": "A while back, David Robinson offered the following advice on Twitter:\nHe also followed it up with an excellent blog post aimed at aspiring data scientists. I think that the most important idea from that post is actually from a presentation he gave. Here’s the key idea, as reported by Amelia McNamara:\nIn summary, things that you keep to yourself have very little value, and things that you share with the world have a lot of value. I think that’s a very cool idea for the Economics of information. For an extreme case, imagine you have a trade secret that allows you to solve a specific class of problems faster than others. In this case, it’s understandable that you want to keep the trade secret to yourself. But it makes sense to advertise to the world that you can quickly solve some kinds problems faster than others. That makes your trade secret more valuable."
  },
  {
    "objectID": "posts/why-you-should-blog-if-you-are-a-data-scientist.html#the-discipline-of-writing-about-what-youre-doing",
    "href": "posts/why-you-should-blog-if-you-are-a-data-scientist.html#the-discipline-of-writing-about-what-youre-doing",
    "title": "Why you should blog if you are a data scientist",
    "section": "The discipline of writing about what you’re doing",
    "text": "The discipline of writing about what you’re doing\nWhen I was a wee little kid and had just entered college, one of my first classes was “Physics Lab”, and the first class of that was to measure “gravity”, more specifically, the gravitational acceleration . Of course, mathy Computer Science studies that we were, we all knew that the gravitational acceleration would be \\(g \\approx 9.8 m/s^2\\).\nMeasuring it, however, it’s not very easy. First, remember that this is in the early 90s in Brazil, so digital cameras were very rare. Part of the problem is that 9.8 meters (approximately 30 feet) is quite high, so if we wanted our experiment to take around 1s, we would need a big ladder. Or, as it happened, we’d need to run an experiment that took less than a second for each run. In a class with 25 students, that was the preferred route.\nWe set up a vertical track attached to a device that would spark every 1/60 of a second, and we attached grid paper to the track. This is called a Behr free fall apparatus.\nWe would release the device from the top of the track, and the sparks would mark the grid paper. We would then manually measure the distances between the sparks and the differences between the distances would tell us the acceleration. In theory. Again, remember this is the early 90s, so there’s no Windows 95 or Excel easily available, there were several steps that were prone to error.\nThe desired outcome was not only to calculate the acceleration due to gravity, but also to generate a lab report."
  },
  {
    "objectID": "posts/why-you-should-blog-if-you-are-a-data-scientist.html#writing-the-lab-report",
    "href": "posts/why-you-should-blog-if-you-are-a-data-scientist.html#writing-the-lab-report",
    "title": "Why you should blog if you are a data scientist",
    "section": "Writing the lab report",
    "text": "Writing the lab report\nIt was a simple experiment, but there was a twist. Another class of 25 students would have to replicate the experiment following the lab report from the first 25. Oh boy. We quickly found out that the best lab reports were the ones in which the experimenter would document the experiment as they were executing the experiment. Another thing that worked was going through the experiment more than once. What did not work was to perform the experiment and then go to where the computers were to type up a report from memory.\nI thought that was very insightful, shortly afterwards, we would learn about Donald Knuth’s proposed paradigm of literate programming, which lives on in Jupyter Notebooks and R Markdown and that is very popular in data science today. Behind it is the same concept of explanations interspersed with technical work to produce something that is reproducible and easy to understand."
  },
  {
    "objectID": "posts/why-you-should-blog-if-you-are-a-data-scientist.html#blogging-helps-you-train-to-write-about-your-work",
    "href": "posts/why-you-should-blog-if-you-are-a-data-scientist.html#blogging-helps-you-train-to-write-about-your-work",
    "title": "Why you should blog if you are a data scientist",
    "section": "Blogging helps you train to write about your work",
    "text": "Blogging helps you train to write about your work\nBesides the excellent reasons offered by David Robinson for aspiring data scientists to blog, I think experienced data scientists can also benefit from blogging. David argues that aspiring data scientists should blog for technical practice, to build a portfolio, and to get feedback. Experienced data scientists might benefit for similar reasons, but not exactly the same.\nSimilar to “practicing”, blogging about technical topics helps you learn how to communicate better about them, and also helps you learn how to write better research reports. Similar to “portfolio”, it signals a data scientist’s body of work. For an early-in-career data scientist, it may be more showing that you can do analysis, but for an experienced data scientist is more about what type of analysis you like to do. Finally, I think that “feedback” also works differently for early-in-career and experienced data scientists. I’m not sure random strangers in the internet will sweep in and correct your posts, but you can find out what people are reading and sharing. You may be surprised when you learn what parts of your work people find interesting."
  },
  {
    "objectID": "posts/the-day-somebody-stole-my-work.html",
    "href": "posts/the-day-somebody-stole-my-work.html",
    "title": "The day somebody “stole” my work",
    "section": "",
    "text": "Part of the work of data science is to create machines that people can use to solve problems. Sometimes these machines can be re-used for other problems with little need for reconfiguration. Sometimes the people reusing one of the machines a data scientist creates will do so without letting the data scientist know until there’s a problem.\nTo protect the innocent, some details of the stories below have been changed."
  },
  {
    "objectID": "posts/the-day-somebody-stole-my-work.html#the-chatbot",
    "href": "posts/the-day-somebody-stole-my-work.html#the-chatbot",
    "title": "The day somebody “stole” my work",
    "section": "The chatbot",
    "text": "The chatbot\nYears ago, I wrote a chatbot to automate some really mundane question-answering for my team. Although it was not something extremely high-tech, it was the exact right tool for the job. Most chatbots I’ve seen are “Rube Goldberg machines”, like the “Self Operating Napkin” below. My chatbot really cut down on mundane manual work, and therefore was really successful.\n\n\n\nRube Goldberg Self-Operating Napkin\n\n\nBecause of its success, my chatbot got a lot of use and it started to be shown to customers as an example of our usage of technology. “Some guy in some place”, let’s call him G, became a specialist in demonstrating the chatbot I wrote, and he would do so very well.\nAfter a while, somebody had a question about my chatbot in an internal distribution list. The question went like “Can that chatbot do X?”. I answered that “No, it can’t do that - that was something that I had thought about but ended up never implementing it”. In a monumental display of corporate civility, someone replied over me saying: “You should ask G, as he is the creator of that chatbot”. I actually declined to further the discussion, and to my knowledge everybody on that thread still thinks that G is the creator of the chatbot.\n\n\n\nchatbot\n\n\nThat made me realize that G was not putting a lot of effort in attributing my work."
  },
  {
    "objectID": "posts/the-day-somebody-stole-my-work.html#the-recommendation-system",
    "href": "posts/the-day-somebody-stole-my-work.html#the-recommendation-system",
    "title": "The day somebody “stole” my work",
    "section": "The recommendation system",
    "text": "The recommendation system\nAt some point I wrote a recommendation system to solve a specific problem, let’s call it the “X Recommender”. It was not particularly well written but it did the job: it recommended X with a decent precision. However, by the time I was done, the client (let’s call them C) that needed that problem solved was moved out. I made a note to improve on that system at some point and stashed it.\nA few years later, someone wanted to be mentored by me, and they happened to be a specialist in X. So we started building models around X, and at some point we revived the “X Recommender” to make it better.\n\n\n\nRecommendation System\n\n\nAt this point, people from C came back asking for an “X Recommender”, and they also wanted to learn how to build one. I told them how to build one, step-by-step. They did not succeed in building it for production, but built it enough for a presentation and scheduled time for us to work together to make it ready for production.\nMeanwhile, I presented the very old “X recommender” to a group. A member of the group saw the similarities and asked me why I was taking credit for the work that C had done.\nThat made me realize that C was not putting a lot of effort in attributing my work."
  },
  {
    "objectID": "posts/the-day-somebody-stole-my-work.html#the-consequences-of-not-having-the-work-attributed",
    "href": "posts/the-day-somebody-stole-my-work.html#the-consequences-of-not-having-the-work-attributed",
    "title": "The day somebody “stole” my work",
    "section": "The consequences of not having the work attributed",
    "text": "The consequences of not having the work attributed\nIt’s not like I’m not recognized for my work, quite the opposite. I think the downside is more “societal”, or “corporate-wide”. In some sense, this is like a signalling problem in which a non-expert in a particular work product signals that they’re an expert. If the signal is misinterpreted, problems that the expert could solve may end up being directed to the non-expert, and that may result in solvable problems not being solved.\nThere are upsides, however. One upside is that a lot of trivial problems and issues gets filtered from the expert and solved by the non-expert instead. Another upside is that the non-expert will likely get better over time, if only to maintain the façade. Presumably the gain in skills from the non-expert will be faster than for the expert, so overall the skills for the company overall will increase."
  },
  {
    "objectID": "posts/the-day-somebody-stole-my-work.html#i-actually-dont-have-an-answer",
    "href": "posts/the-day-somebody-stole-my-work.html#i-actually-dont-have-an-answer",
    "title": "The day somebody “stole” my work",
    "section": "I actually don’t have an answer",
    "text": "I actually don’t have an answer\nI’m not sure how to make this better. I have received feedback that I don’t “sell” my work enough, but it’s a complicated equation…\n\n\n\nComplicated math\n\n\nThe problem is that selling the work takes time, and that will sometimes result in less time to do high-quality data science work. It may also result in wider than optimal adoption, which will require extra support, also detracting to high-quality data science work. Perhaps it pays better, but higher-quality also generally pays better than lower-quality, so it’s not that simple.\nOf course, I wish my work was such high-quality that this was the real problem. The likely explanation is that I rather work with models than sell them."
  },
  {
    "objectID": "posts/centralized-vs-distributed-data-teams.html",
    "href": "posts/centralized-vs-distributed-data-teams.html",
    "title": "Centralized vs. Distributed Data Science Teams",
    "section": "",
    "text": "In a recent DataFramed podcast, Hugo Bowne-Anderson and Jonathan Nolis discuss whether data scientist teams should be centralized or distributed. I have my own opinions about that question."
  },
  {
    "objectID": "posts/centralized-vs-distributed-data-teams.html#should-data-science-teams-be-centralized-or-distributed",
    "href": "posts/centralized-vs-distributed-data-teams.html#should-data-science-teams-be-centralized-or-distributed",
    "title": "Centralized vs. Distributed Data Science Teams",
    "section": "Should data science teams be centralized or distributed?",
    "text": "Should data science teams be centralized or distributed?\nI was happy to find that Jonathan Nolis has the same opinion as I do: that a data science team works best when distributed into client areas. In addition, we agree that if you distribute your data science team among several different client areas, you have to find a way of keeping the data scientists feeling as if they’re part of a cohesive group. And that’s hard.\nFor me, a key part of the argument to distribute data scientists in client areas relies on Drew Conway’s (ancient) Data Science Venn diagram.\n\n\n\nData Science Venn Diagram\n\n\nAssigning data scientists to a client area will help build the “Substantive Expertise” part of the diagram. A common trap here is “Well, we can do the same if we keep the data scientists centralized and give them specializations that match the client areas.”\nAlthough I think that can work, I worry about what happens when data scientists have downtime.\n\nIncentives for client areas and centralized data science teams\nIn my observations (low sample size), the client areas usually have a better sense of ROI1 - if they invest resources on something, they expect returns. On the other hand, most centralized data science teams I’ve seen have two problems: data scientists hired without a clear purpose and an incentive to overengineering: a current example is to use Deep Learning where a regression would do.\n\n\n\nOverengineering\n\n\nIt may be simply that client areas are more mature and battle worn than centralized data science groups, at least in my experience. I think that if a data scientist is assigned to a client area and there’s downtime, they’ll put the data scientist to work in business intelligence or decision analysis. On the other hand, if the data scientist is assigned to a centralized data science group, they’ll be pulled into a project away from their “client area specialization” or will find a way to work in something novel that’s cool (as of 2018, autoencoders, GANs or something with “Bayesian” in its name).\nAnd here the ROI appears again: as the data scientist works through the BI project, they’ll learn more about the data and more about the business and its data, and that investment has a large likelihood of return in their next project in the same group. Whereas if they spend this time learning a new technique, the return is uncertain.\n\n\nBut wait, if you do that, data scientists will never learn something new\nHere’s where the hard part of keeping a data scientist community pays off. That community may be weekly “Lunch & Learns” if the number of data scientists is small, or quarterly conferences if the number of data scientists is very big and your company is very, very rich.\nIf you don’t forcefully push towards having such a community, it’s very likely that data scientists will reinvent the wheel over and over again, and if their time is as precious as their high salary indicates, that will be a waste of money. For example, if you have 5 data scientists It’s better to spend $50k on travel to bring them together a few times a year than to have one of them wasting three months reinventing something that another of them already done.\n\n\nWhere I disagree with Jonathan Nolis\nIn the podcast, Jonathan suggests that the data scientists should be assigned to client areas and report “solid line” to a Chief Data Scientist, while reporting “dotted line” to the client areas. I would do the opposite. However, listening to the podcast, it seems that Jonathan didn’t have a strong opinion that one is better than the other, and again in agreement, neither do I.\nOne big advantage of having a “solid line” group of data scientists is career advancement. It’s easier to compare data scientists with other data scientists. Leaving them in the client areas will require to compare them with other members of the client area, and they may be seen as wizards (and overvalued) or “too hard to understand”, and be undervalued.\nOne disadvantage is estimating the value of the data scientist to the client area. Data scientists that are more creative in their value calculations or less scrupulous are probably going to get ahead. I’ve lost count of the times that a data science project saved more revenue than the customer ever had.\n\n\nSo, what do I do?\nHonestly, if you’re starting a data science team, your best bet is to hire someone like Jonathan or Drew Conway to plan the team for you. You’re going to spend a lot of money on Data Science, and it’s important to make sure that it will be well spent.\nIf you’re on a budget and all you can invest is the time to read some blog posts, well… you probably don’t have enough money to hire data scientists. But if you do, I recommend assigning them to the client areas and keeping them there. Even when they’re not doing modeling or machine learning, they’re learning more about the business and its data."
  },
  {
    "objectID": "posts/praising-conway-s-data-science-venn-diagram.html",
    "href": "posts/praising-conway-s-data-science-venn-diagram.html",
    "title": "Praising Conway’s Data Science Venn Diagram",
    "section": "",
    "text": "Drew Conway’s Data Science Venn Diagram is perhaps the most well-known description of data science.\nDrew published it more than five years ago. Since then, a ridiculous number of Venn diagrams have been created, one worse than the other.\nFive years later, people still struggle with the definition of Data Science. A recent famous example was Lyft changing the titles of their Data Scientists, Data Analysts and Research Scientists."
  },
  {
    "objectID": "posts/praising-conway-s-data-science-venn-diagram.html#should-we-care",
    "href": "posts/praising-conway-s-data-science-venn-diagram.html#should-we-care",
    "title": "Praising Conway’s Data Science Venn Diagram",
    "section": "Should we care?",
    "text": "Should we care?\nThere’s a lot of information asymmetry in the Data Science market, the position being so new, and “Michael Spence signaling” matters. Having a “Data Scientist” title from a reputable company is an important signal. For example, a medium-sized company might not be aware that Lyft recently changed their data analysts title to “Data Scientist”\nPart of the problem is that the “Data Scientist” title is itself poorly defined, as shown by the litany of Venn and not-Venn diagrams listed above. A short, non-comprehensive list of alternative titles for Data Scientists I’ve met in the last month:\n\nTableau/Power BI Report Designer\nMachine Learning Researcher\nPython Programmer who knows how to call scikit-learn functions\n\nUsing the same name for vastly different abilities makes matching between employers and employees difficult. In addition, it may make career progression difficult. For example, job responsibilities and objectives are likely to be related to a company’s interpretation of the title of “Data Scientist”. Employees in that company may be “Data Scientists” under other interpretations, and may be really good, but may have their performance evaluated under different guidelines: “I know your ML model saved $1B for our company, but the color scheme of your Tableau reports is horrible!” As per the (not really) Albert Einstein’s quote “(…) if you judge a fish by its ability to climb a tree, it will live its whole life believing that it is stupid.”"
  },
  {
    "objectID": "posts/praising-conway-s-data-science-venn-diagram.html#conways-venn-to-the-rescue",
    "href": "posts/praising-conway-s-data-science-venn-diagram.html#conways-venn-to-the-rescue",
    "title": "Praising Conway’s Data Science Venn Diagram",
    "section": "Conway’s Venn to the rescue!",
    "text": "Conway’s Venn to the rescue!\nHere’s where I think Conway’s Venn diagram comes to the rescue in a useful way. We could rule out the Tableau Designer if they lack programming or statistical skills. We could rule out the ML Researcher if they don’t have domain expertise. We could rule out the Python programmer if they don’t know Math and/or have domain expertise. Perhaps this makes Data Scientists hard to find (it would justify their salary)!\nDepending what the company you work for was before the Data Science profession started, a lot of people from different jobs may have re-branded themselves as data scientists. I think Conway’s diagram is helpful as it points to where people should focus their development in order to be more effective as data scientists. For example, former Economists most likely need more hacking skills, former computer scientists need more Statistics and domain expertise, and former management/technical consultants likely need more Statistics.\nIt can also serve as a check for inclusiveness. For example, a team of former machine learning engineers is struggling to deliver meaningful projects because they lack domain expertise. However, they can’t hire domain experts because the domain experts keep failing the machine learning interviews. Comically, the machine learning engineers think that domain expertise “is easy”.\n\n\n\nEverything you don’t know is easy\n\n\nThe most useful part of Conway’s diagram for me is that I’ve time and again observed projects that don’t have representatives that can communicate well with each other in all three areas to fail. Conway’s diagram even makes it easy to show how the project failed (“nobody with domain expertise”, “missing knowledge of statistics”). Having a single person that can do all three is difficult, but it certainly solves the communication problem. That being hard or impossible, having specialists that can communicate with each other will usually be sufficient."
  },
  {
    "objectID": "posts/praising-conway-s-data-science-venn-diagram.html#but-conways-venn-is-missing-something",
    "href": "posts/praising-conway-s-data-science-venn-diagram.html#but-conways-venn-is-missing-something",
    "title": "Praising Conway’s Data Science Venn Diagram",
    "section": "But Conway’s Venn is missing something!!!",
    "text": "But Conway’s Venn is missing something!!!\nTrue to the famous Box aphorism that “all models are wrong but some are useful”, Conway’s Venn is not perfect. In my experience, one of the things that I usually miss is “intensity”. Someone that knows how to write a “Hello World!”, knows how to take averages, and know how to calculate compound interest has knowledge in all three areas of the Venn diagram, but is probably not the ideal person to lead a data science project in Finance. Conway’s is best used to tell you what’s missing for career development, for a team, for a project, but it will not solve all problems.\nIt will, however, solve many problems that matter, and will provide useful information very elegantly. You can’t ask much more from a model."
  },
  {
    "objectID": "posts/lenovo-p51-the-data-scientist-laptop.html",
    "href": "posts/lenovo-p51-the-data-scientist-laptop.html",
    "title": "Lenovo P51 - the Data Scientist’s laptop",
    "section": "",
    "text": "The Microsoft Surface Pro 3 was the right computer for me at the time I got it, but as time went by my position changed substantially, and it ceased to be appropriate for me.\nWhen I got the Surface, the core of my work consisted in traveling, having conference calls and light emailing and spreadsheet work. As time went by, my work went back to creating analytics, creating presentations about analytics, and traveling. There is where the screen of the Surface becomes a liability at 12 inches. I wanted something bigger.\nAnd boy, is the P51 bigger. It has a 15-inch screen and a power brick that alone is twice as heavy as the Surface Pro. It’s big enough to have a keyboard that comes with a numeric keypad. Since I still have to travel a lot, I chose the P51 over the beefier 17-inch P71. The P71 won’t even fit in my somewhat large Osprey backpack. The P51 barely fits.\nDue to some standardization constraints, my P51 is the 1920x1080 version with touch screen. It has 32 Gb of RAM and a 500 Gb SSD. It has the i7 7820HQ running at 2.9 GHz and a Quadro M2200 GPU. If I had the option, I’d choose the 4K screen and the Xeon processor, but I did not have that option.\nThe battery life is surprisingly good. For the kind of things that I do in planes (writing blog posts, editing presentations, light programming), the battery seems to last well over 8h without Wi-Fi and about 7h with Wi-Fi. I have yet to run out of battery in a flight. I have read that the earlier version of the P51, the P50, would trip the circuit breaker of a plane if you tried to plug it in. I have not attempted to plug in the P51 yet.\nThe P51 is usable in the Premium Economy seats and, of course, in business and first class seats, but I have the impression that it would not fit well in the economy class of carriers that have a very small distance between seats (like American Airlines domestic flights), especially if the person in front of you reclines their seat. It works well in exit seats.\nOne gripe I had about the Surface Pro 3 was that I always had to travel with a USB hub, a mouse, and a couple mini-DisplayPort adapters. The P51 comes with a mechanical keyboard, the beloved TrackPoint, a good TouchPad, 4 USB ports, a Thunderbolt 3 port, an Ethernet port, an HDMI port, and, to my surprise, a mini-DisplayPort, so I get to keep all the adapters I had for the Surface.\nI’ve also considered the P51-S. The slimmer version trades off some power (it goes from quad-core to dual-core) for a thinner, lighter frame. I think the P51-S would also have been a solid choice, but I think it would force me to do most of my work from a workstation and/or a server that I’d need to remote into frequently, adding to the stress of versioning and synchronizing. Somewhat counter-intuitively, I chose the heavier computer because I travel a lot, as it has enough power for me to do the vast majority of my work on it and use a server or workstation just for the real hardcore stuff.\nIn summary, the Lenovo P51 is a great computer for the traveling data scientist. It’s powerful enough to be used as your primary machine. For me, it’s near the top limit of a computer that is comfortable to travel with, but still within that limit. I got used to it in no time and completely gave up my desktop workstation."
  },
  {
    "objectID": "posts/hurricane-irma.html",
    "href": "posts/hurricane-irma.html",
    "title": "Hurricane Irma",
    "section": "",
    "text": "Hurricane Irma happened when I was out on a business trip, and my family was in Parkland, FL. Forecasts went from “maybe it will hit you” to “if you stay you’ll surely die” to “I think it’s not going to hit you anymore” to “Phew, it will miss” in four days.\nWe did a lot of preparation using conditional probabilities. In our assessment, it would be very unlikely for the hurricane to hit both the East and West coast. So we decided to move the family from Parkland, FL to Tampa (back then, the hurricane was forecasted to hit the East coast), but to book hotels in both Pensacola (Florida panhandle) and Jacksonville, FL while we waited for new forecasts.\nAs days went by, the forecasts moved further and further West. On Saturday 2 AM, the forecast was that the hurricane would hit Tampa as a Category 4, so staying there was too risky. And then we had to decide between moving to the Florida panhandle, moving to Jacksonville and incredibly, going back to Parkland, near Fort Lauderdale.\nThe uncertainty seemed higher for places that were north of Tampa: it was hard to tell whether the hurricane would turn either way in 3 days. On the other hand, the forecasts are better in close proximity to the hurricane, and it seemed pretty certain that the hurricane would miss the Parkland/Boca Raton area. So we made the somewhat gutsy call that my wife would come back to Parkland with the kids.\nThere’s surely an element of luck to all of this, but the outcome was positive. We had very minor damage in our house: a couple plants and a downspout. I think the total amount of damage is going to be less than $100. It’s hard to be precise because there may be damage that is hard to see, like on the roof. We did not lose power, internet or even satellite TV. The hardest part was the stress.\nOn Monday morning the winds were back to normal. There’s still a lot of debris around the county. I’m supposed to fly back home tomorrow, if the airport opens, and hopefully I’ll be able to get a ride home. This was more of a suspense than a horror movie. Many people were not as lucky as we were and we’ll now see how we can help them."
  },
  {
    "objectID": "posts/installing-arch-linux.html",
    "href": "posts/installing-arch-linux.html",
    "title": "Installing Arch Linux",
    "section": "",
    "text": "Even though I work at Microsoft and really love Windows, I’ve recently installed Arch Linux as a dual boot.\nA lot of data scientists and software developers I know prefer OS X, and Windows doesn’t get that much love. Although many people provide many reasons not to use Windows, I was always able to work around whatever those issues were. Sometimes this required Cygwin, Git Bash, and more recently, WSL (the Windows Subsystem for Linux). I can’t remember any software that I was ever unable to use in Windows: LaTeX, git, Inkscape… everything I need had a decent Windows version, even if I had to compile it from source.\nWeirdly, because I can do so much with Windows, I find that I take too many shortcuts. The most damaging shortcuts are the ones that damage reproducibility, especially when creating documentation for my projects. It’s too easy to copy and paste into PowerPoint and be done with the communications for a talk. Some of my talks, especially about my most successful product, have many similar versions with one or two slides changed. Someone asks me for a slightly shorter or slightly longer talk and I suddenly have a new file with one or two extra (or hidden) slides that I may need to maintain. It’s not lost on me that this is a lack of discipline on my part, but if you were to invite me to a brigadeiro store, it would be hypocritical of you to accuse me of having no self-control."
  },
  {
    "objectID": "posts/installing-arch-linux.html#why-arch",
    "href": "posts/installing-arch-linux.html#why-arch",
    "title": "Installing Arch Linux",
    "section": "Why Arch",
    "text": "Why Arch\nI first got into contact with Arch through a German friend back in 2010. When trying to install it, I quickly learned that Arch requires you to understand what you’re doing, and especially, that if you don’t understand what you’re doing, things break. Over time, I found that although some things have a steep learning curve, the return on the time investment was very positive. I also tried Ubuntu, Fedora and Red Hat, and although I could get productive more quickly, keeping them configured to my taste later was harder - when something broke it took me longer to understand what it was.\nThere are several things that still don’t work well in Linux. One of the most vexing is having a high-DPI display. It’s somewhat hard to configure it. It works well in KDE, but KDE has tearing issues. It doesn’t work too well in GNOME unless you use Wayland, which brings a host of different issues. You can make everything a lot worse by using the NVidia proprietary driver with Wayland, but it’s needed if you want to use the GPU efficiently.\nSo although I managed to configure my Arch installation to use GNOME under Wayland and use the NVidia proprietary driver, it’s very unstable. Native applications tend to run, but many applications crash in comical ways: resizing Google Chrome under Wayland is a sure way to cause a crash, for example."
  },
  {
    "objectID": "posts/2022-06-10-jekyll-to-quarto/index.html",
    "href": "posts/2022-06-10-jekyll-to-quarto/index.html",
    "title": "Converting my blog from Jekyll to Quarto",
    "section": "",
    "text": "What I wanted in a blog platform\nFor a long time, I had an unfulfilled wish list for a blog platform:\n\nCan write posts in Markdown\nEasy to deploy to Github\nCan script in Python\n\nA large number of platforms fulfills the first two items, but I struggled to find one that fulfilled the last item. I finally found Pelican, but I found it too clunky and started to worry that it was not going to be maintained for long. Therefore, I oscillated between Jekyll and Hugo (using blogdown) for a long time. I liked blogdown a lot, but whenever I’m switching a lot between R and Python, my brain gets really unhappy.\n\n\nQuarto has everything, and more\nRecently, I found Quarto that ticks all my boxes and solves a lot of other problems. My feeling when using Quarto must be similar to the feeling of Python programmers when they started using Jupyter (because they never used RStudio) or similar to what I felt when, as an analyst, started using RStudio. Things just work.\nWhen I was initially switching between Jekyll and Hugo, one of the most annoying things was Jekyll preferred posts as files, while blogdown preferred posts as directories. RStudio even provides a function bundle_site() to convert files to directories. Now, when I’m converting to Quarto, I thought I needed to write such a function in Python.\nAgain, it turns out that “things just work”. Quarto accepts both directores and files as posts and rendered everything correctly. So I’m going to draw the “Quarto Spiral” instead, just to celebrate!\n\n\nExample from Hello, Quarto!\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/2022-06-09-moved-linkedin-posts.html",
    "href": "posts/2022-06-09-moved-linkedin-posts.html",
    "title": "Moved all my LinkedIn posts",
    "section": "",
    "text": "I moved all my LinkedIn posts that were on the blog to a new section."
  }
]